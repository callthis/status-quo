<div align="center">
  
  **[:arrow_left: Previous Section][Prev] | [Table of Contents][TOC] | [Next Section :arrow_right:][Next]**
  
</div>

---

## 1.3 Automated Discovery and Exploitation of Zero-day Faults

For this mathematics and computer science based topic, I will first describe a few definitions, and present an example that introduces the challenges involved in automated detection of human faces within a video. 

The presented example is for giving the reader an idea about the difficulty in automatically "recognizing", any conceivable "concept", in any possible media and ecology, within which an Artificial Intelligence (AI) based system can co-exist and function, along side beings with general intelligence. 

The computationally useful and novel things produced by me for the endeavor of automatically parsing any given system's description by an AI, are buried within tomes of text in this article. All of the seemingly superfluous content and opinions are included here, because they are to create the experience of excavating "nuggets of treasure" for the reader, as if by chancing upon them through an archaeological digging of this heap of text and images.  

Ultimately, this section aims to describe an automated method for discovering aspects of any given system, that constitute a "fault", and that too a zero-day fault, to subsequently generate automated solutions for patching them by replacing the faulty system with a better, automatically generated architecture; or for exploiting the identified faults, based on morality and ethics of learned commercial, industrial, and military purposes and activities. 

How can building that type of an advanced AI, not turn into a bane for humanity? Well, we will just have to figure out answers to such concerns using advanced AI that can evaluate itself, other AI, as well as non-AI based systems presented to it, including ones that it finds autonomously, because doing all of that manually for globally networked industries is not feasible, within one mortal lifetime of any human being. 

Would putting more "men" on the job, achieve the required goal of answering difficult questions pertaining to AI ethics? Maybe. But how would one find and task sufficiently many people from all walks of life, and genders, to contribute to answering such questions at a publicly accessible platform? 

Wait, wouldn't building an advanced AI and deploying it into an industrial environment alongside unassuming human workers, for evaluating whether another computerized system in its ecology is or isn't harmful, in itself, likely to become uncontrollably problematic? Uhhh... yes, sure, why not? But, let's not assume the worst, and simply go ahead with this project using the type of astronomical levels of unbridled optimism that is typically endemic to "pioneering" and "entrepreneurial" ventures; all of which, is supposedly an essential part of the free-market regulated and well-balanced impetus needed for: 

- "making 'the world' a better place", and, 

- "protecting the brave and democratic [Free World](https://www.atlanticcouncil.org/programs/scowcroft-center-for-strategy-and-security/global-strategy-initiative/democratic-order-initiative/commission-on-advancing-a-free-world/) from 'naysayers'." 

By the way, if you can identify something within yourself, or in your environment, that is "not a system", then, please do contact me, by submitting a pull request with your 'educational' article or content, regarding the concept of "not-system", which can then be added to this repository in a collaborative manner. Or you may join the online discussions here - https://github.com/orgs/callthis/discussions 

**Note:** All contents of public discussions pertaining to the GitHub "Org" named "callthis" are attributed the license of Creative Commons Zero, Version 1.0 (CC0).


<details><summary><h3>1.3.1 Preliminaries</h3></summary> 

Let "point" is a thing such that, it has no further parts, not even a causal factor that generates or justifies its existence. 

The above definition of a point from Euclid's Elements, turns out to be one of the most brilliant axioms that any human being could have ever discovered, because it forms the basis of all geometry, and is an intrinsic part of every aspect of spacetime as well as all other gamuts. Such a definition of a point also indicates that it is whole within itself. Furthermore, due to its imperceptible form and infinitesimal nature, it is an abstract mathematical entity. 

One can also assert that infinitely many points exist between any two adjacent points. But, as human beings, we are compelled to use a "dot" to represent a point, when discussing it in the finite context of a tangible medium such as a piece of paper, a chalk board, a cuboid, or an ellipsoid. 

Now, any "collection" of points can be termed as a locus. A straight line is a locus in which the collection of points share a property called linearity. Any line, along which and about which, a "set" of points is distributed, is called a geometrical axis. The number of mutually "orthogonal" axes needed to unambiguously index each point within a generalized "space" indicates the "dimensionality" of that space. The maximum and minimum span that can be measured within a space, respectively define the maximum "size", and the "least count" of mathematical operations that produce a measure within, or from the space being taken into account. 

In mathematical literature about machine learning, the concept of "dimension", of a given matrix, is often confused with the "dimensionality" of the matrix. Also, in literature about machine learning, the number of dimensions in a trained model, usually alludes to the number of "features" obtained via permutations of all the elements available in the training and test datasets. 

So, to avoid confusion, let us instead use the words "size", and ["tensor rank"](https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)#Tensor_rank), to describe a set of points, elements, data, or items that can be represented as a matrix, whereby: 

- The size of a given matrix is the total number of elements within the matrix. 

- The tensor rank of a given matrix coincides with the total number of ["Eigenvalues"](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Calculation) of the dataset described within the matrix. 

</details>

<details><summary><h3>1.3.2 An Example from The Science of Image Processing</h3></summary>

Here is an example to highlight difficulties that will be encountered in getting to automated concept recognition, by looking at the topic of object recognition with a trained machine learning model that [detects human faces](https://en.wikipedia.org/wiki/Face_detection) within a RGB-color video. 

The contents of a RGB-color video, might be described using a matrix that could have a so-called dimension of billions. Billions of what, you ask? That kind of a question only tends to upset some of the machine learning scientists and engineers who build machine learning models, which then causes them to fall into an argument about whether the word dimension of their training algorithm's output matrix alludes to its size, or to its maximum number of columns. You will be able to see that, a dataset containing a RGB-color video only has elements distributed: 

- along one axis describing the color channel of a pixel; 

    - the integer values on this axis indicate a number corresponding to a color channel, that is, red, green, or blue

- about two more axes for describing the position of a pixel in an image frame 

    - the integer values with respect to these two axes correspond to a coordinate of a pixel, where each pixel is bounded within the size of the image frame, say, 1080x960 pixels 

- along one more axis for the light intensity at a given pixel location indexed by the above three axes 

    - a pixel's intensity is typically an integer value between 0 and 255, for images that use 32 bit data in an image container format, like [JPEG](https://en.wikipedia.org/wiki/JPEG#Typical_use) 

- and along one more axis for describing the time-stamp of each image frame in a video container format, like [MPEG](https://en.wikipedia.org/wiki/Moving_Picture_Experts_Group)

    - this integer value is dependent on the least count of the clock used for timing the video, which could be in milliseconds, resulting in, say, 60000 frames for a one minute video  

So, in the above example, the size of the dataset would be 3x1080x960x256x60000 = 4.7775744e+13 *pixels.* 

To then say that you have a training dataset matrix with approximately 47 billion dimensions would be silly, because regardless of the change in the number of video frames within a video file, any pixel in the dataset would be unambiguously indexed with only five pieces of knowledge about its location and intensity. As such, the video data would be distributed in a space constructed using five axes, that is, a geometric space having a tensor rank of five. 

A trained machine learning model for face-identification could be a matrix with a very large size, due to a particular permutation of, say, [Haar-like Features](https://en.wikipedia.org/wiki/Haar-like_feature) used for describing a human face to the [Viola-Jones algorithm.](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework) However, that type of trained model or template for face-identification, can use the intensity values of a single color channel, for each two-dimensional video frame. Thus the data in the trained model would instead be described by a space constructed by three axes for pixel locations within an image frame, and one axis for the time-stamp of each image frame in a live video stream. 

That trained model or template, can then be iteratively matched with "chunks" or sections of each test image frame, to be able to identify any geometry that resembles a human face. Naturally, the amount of computational resources needed for identifying faces within a live video stream, would be quite expensive.

So, imagine doing the above type of "feature-identification" within live audio and chat data-streams alongside live video data, for building an interactive robot; a robot which can understand as well as interact with the world around it, in a manner that is as intelligent as a college educated adult human being, if not better. Even without haptic and olfactory data channels, merely audio and visual channels expressed as electromagnetic signals to a robot, to make it as "self-actualized" as a college educated adult human being, may seem like a physically impossible or infeasible project. And yet, one needs to remember that something like the products being created by Google or Baidu, already outperform most human beings on various well-defined tasks like driving a car in a highly regulated environment. 

```
Computational resources have become affordable, and the ability to incorporate new developments
into one's own project has continued to become more accessible to technologists.

Here is an example of object detection within a live video stream using Google Coral:

Live Object Detection at 70FPS with low cost hardware - https://youtu.be/T-VjYr7sZC4?t=123 

```

Thus, the process of merely identifying building blocks or factorized components, of a digitized data-stream from "the real world", which resemble labeled items in a training dataset, isn't the required final output of a robotic system that is meant to approach "general intelligence." The algorithmic procedure for making the AI recognize semantic meaning out of a string of features within a data-stream, to then "autonomously" act upon "the knowledge and understanding" obtained from that process, firstly requires a [Large Language Model](https://en.wikipedia.org/wiki/Large_language_model). 

From philosophies of mind and natural language, as taught in Westernized universities, oral or written human speech that makes use of "words" or "symbols", not only conveys pre-formulated and formatted intent of the communicator, it also conveys the "intentionality" that the communicator experiences, during the act of performing activities in-line with the communicator's motives and desires. Said intentionality is supposed to be the "feeling", or human experience, of wanting to continue or to halt an ongoing activity, based on real-time sensory feedback from the communicator's environment. That philosophical approach to describing linguistics and cognition is drastically limited. So, let us generalize the idea of "language" from its gestural, or behavioral form, to a cascade of synchronous as well as asynchronous articulations that can provide "signals" from a sender to a receiver, via the physical process of "communication." 

>The most basic definition of physical communication is: "transfer of 'information' from one system to another via signal transmissions, energy transduction, or changes in momentum or electromagnetic states of the interacting systems via 'contact', at any possible span of short or long distances in spacetime." 

At least, that is how I like to define physical communication. Also, changes in momentum or electromagnetic states necessarily entail thermodynamic changes among interacting systems, that are causal, and not merely coincidental. 

What then is a system, and how are system boundaries or interfaces defined? Well, only "nothingness" is not a system, and the chore of identifying dynamically evolving boundaries of a system, requires the use of ["Ontological Methods in Systems Engineering."](https://github.com/callthis/status-quo/blob/main/docs/01-02-07.md#1271-ontological-methods-in-systems-engineering)

So, let us construct a topological space that describes the mathematical relationships between all conceivable concepts, with those concepts being distributed upon the constructed topology, including the concept of a concept, the concept of recursion, and even the concept of what a topology can be in terms of a mathematical object of study. In doing so, we will build an algorithmically generated "semantic model", instead of a "language model." The semantic model could then be utilized for tasks like discovering and studying mathematical theorems, that have yet to be recognized using human efforts without the aid of generative-AI based analytical engines.  

</details>


<details><summary><h3>1.3.3 A Topology of Semantic Models</h3></summary> 

As shown in the following diagram, ***Line AB*** represents the axis defined as "the part-whole continuum", and ***Line AD*** represents the axis defined as "the abstract-tangible continuum." The 2-Dimensional ***Area ABCD,*** represents a gamut of concepts known as an ["abstraction hierarchy."](https://github.com/my-realm/oc/blob/master/doc/ah.md#history-of-abstraction-hierarchy) Each point on this gamut is indexed as the coordinate of an individuated concept, such that each of those concepts are comprehensible to human beings. The semantic meaning ascribed to each concept on the gamut, can thus, only be relatively abstract-or-tangible with respect to a subjective observer; and is simultaneously, either a component or an ensemble, in relationship to other concepts located on the gamut, using the objective basis of "set theory." 

This 2-Dimensional gamut of concepts can then be converted into a 3-Dimensional model, by introducing an axis that represents the continuum of "depth-feature versus surface-feature." 

So, to make the corners of the 2-Dimensional ***Area ABCD*** touch at a single ***Point E,*** "fold" the area as shown in the following diagram, to create a 3-Dimensional volume that represents, "a semantic space."[^1]  

<p align="center">
    <img width="65%" src="../imgs/semantic_space-3d-model.png"></img>
    <br>
    <b>Converting the 2-D model of an "abstraction hierarchy" into a 3-D model of a "semantic space."</b> 
</p>
<br>

In the newly obtained 3-D model of a semantic space: 

- Depth-feature is a thing, or a concept, that is closer to the sub-atomic description of reality in terms of physical spacetime measured in spans of [Natural Units](https://en.wikipedia.org/wiki/Natural_units) (such as but not limited to: [Planck units](https://en.wikipedia.org/wiki/Planck_units), [Stoney Units](https://en.wikipedia.org/wiki/Stoney_units), and [Fine-Structure Constant](https://en.wikipedia.org/wiki/Fine-structure_constant)), in comparison to the macro-level, biological shape and size of human beings. 

- Surface-feature is a concept, or a thing, that is more readily accessible to unaided human biological sensory organs, and thus to human cognitive faculties, without the aid of technological instruments, or tools and gauges. 

- And the topic of abstract-versus-tangible can be made more objectively measurable, by comparing all given concepts, including things like "time", or "chair", with the most abstract concept called "point." Let the origin of the new 3-Dimensional coordinate system containing the model of a semantic space, coincide with the **Point E.** The origin of the coordinate system is also supposed to represent the location of the geometrical concept of "a point." All other concept placed on the 3-Dimensional topology are to be measured at a distance with respect to the origin using vector algebra. How to do so will be made clear in the subsequent sub-sections. 

---

<details><summary>By the way,</summary> don't try comparing things to God, either via similarity or via contrast, because all forms of comparison of a conceivable thing with God, results in that created thing ultimately becoming, annihilated.</details> 

---

Anchoring concepts to locations on a 3-Dimensional topology called the semantic space, using a regularized arrangement that can be codified, highlights causal relationships due to which, causality is directed from a relatively abstract depth-feature to a relatively tangible surface-feature. Subsequently, any feedback loops that can exist in a vectored manner, directed from a relatively tangible surface-feature to a seemingly abstract depth-feature, can also be discovered via the encoding method that is described in the next sub-section of this article. Therefore, causality exists within ecological feedback loops, and isn't something that is somehow directed strictly from "the human mind", onto "the material world." 

So, one must first ask themselves, are there any combinatorial arrangements of physically measurable quantities of the universe we exist in, that are capable of accurately and precisely describing concepts like personhood, cognition, intelligence, awareness, spirituality, attention, willingness, wellness, aesthetics, morality, ethics, veridicality, legality, meaningfulness, or contentment? 

Also, do people need to define concepts like mind, or a soul, the way the concepts of point, or the average [speed of light](https://en.wikipedia.org/wiki/Speed_of_light) between two points within "isotropic" "free space" have been defined, in order to be able to express themselves as mere human beings; or to be able to construct scientific tools and equipment for experiencing a better quality of life, while utilizing other well-defined concepts via arts as well as engineering and managerial sciences? 

Most importantly, what kinds of concepts are yet to be properly defined or even discovered, that would otherwise make existence more sensible and worthwhile, for human beings as well as other living species?  

</details>


<details><summary><h3>1.3.4 Intuitive Method of Constructing a Semantic Model of a Given System</h3></summary> 

We can take a look at an intuitive form of the pseudo-code for creating a machine-readable model of a semantic space, using the following diagram labeled as "Principled Thinking."  

<br>
<p align="center">
    <img width="65%" src="../imgs/Principled-Thinking.png"></img>
    <br>
    <h4 align="center">Principled Thinking</h4> 
</p>
<br>

The above diagram may better explain the process of discovering "blind-spots" and "misconceptions" within any system's design, for persons who are less inclined to using an algorithmic description of how to construct a machine-readable semantic model of a system. The diagram depicts four stages of analyzing any given system, in which each stage must be in concordance with the adjacent stages.  

- To begin the analysis of, say, an existing grocery store, the analyst would start at Stage 1, concerning the store owners' ethos and cultural values. The reason for owning and operating a grocery store would be derived from the cultural values and needs formally stated by the store's stakeholders. The roles and responsibilities that the company's employees are expected to perform, as well as the standards of customer satisfaction and yearly profits they are to aim for, are contingent on the cultural values that company members can adhere to. If concepts like "honesty", "customer satisfaction", "employee code of conduct", "facility's cleanliness", "handicap parking and accessibility", "building's architectural appeal", "product visibility", or "safety" weren't a primary concern, then the analyst would be able to identify such deficiencies before moving onto the next stage. Stage 1, provides answers to questions starting with a "why." For example, "Why is a security system needed in a grocery store?"

- Stage 2, of the store's analysis would identify the financial and physical resources available that match the principles and policies of the store's proprietors. In this stage the analyst can identify underutilized assets as well as shortages within required resources that need to at least, meet the value system and sense of aesthetics subscribed to by the company's stakeholders. Stage 2, provides answers to questions starting with a "what" or a "which." For example, "What kind of a neighborhood is the store located in? Which type of financial, physical, and digital security measures are available or needed, to maintain desired level of operational safety and 'peace of mind'?" 

- Stage 3, of the analysis provides answers to questions beginning with a "how." For example, "How is a particular version of financial, physical, and digital security system to be implemented, with the necessary administration of employee training and corporate policies?" This stage of analysis, conjoined with the previous stages, would highlight key areas of strengths and weaknesses in the day-to-day managerial operations of the grocery store. Any new polices that need to be created, and any physical resources that need to be reallocated or additionally acquired, are to be clearly identifiable during this stage. This is also the stage where business operators would be able to analyze and ratify, human resource policies, inventory management schedules, advertising campaigns, salaries and wages, product pricing strategies, and investor relationships, to arrive at statements about sales and profit goals. 

- Stage 4, which is an abstract-whole, turns out to be a natural outcome of the previous stages of "thinking and making", whereby the company's goals become defined in "achievable and realistic" terms. Any envisioned goals without taking stock of available skills and resources along with a proper grounding within the company's espoused cultural values, would most likely turn into a pipe-dream. Coming up with fancy goals and mission statements that possibly cannot be achieved within the constraints of available ground truths, can thus be avoided. Ultimately, Stage 4 of the analysis, must be in accord with the axiomatic principles identified at the very onset of the business analysis. 

The systematic process or "series of well-formulated steps" described here, isn't an algorithm for automatically discerning "truth", it is an algorithm for indexing knowledge discovered via interactions with a given system, tabulated in the form of a particular type of a graph, in order to excavate what else needs to be discovered and then built, towards materializing required physical outputs from the given system.

You may thus note that, Stage 1 of the analysis identifies *abstract-parts* of the given system. These abstract-parts are the principles, norms, and needs, that constrain or bound the system within a real ecology, in an axiomatic or "legally" defined way. Stage 2 of the analysis describes a set of *tangible-parts* of the system, pertaining to existing materials and processes that can be utilized as per known constraints. Then, Stage 3 produces a set of performance indices and engineering designs in the form a *tangible-whole,* which describe how certain kinematic and dynamic relationships can be connected or built, to arrive at viable goals using the concordance between Stages 1 and 2. 

The eventual Stage 4, reifies the outputs of the system in a descriptive manner, by showcasing its functionality and material qualities in comparison to the ecological constraints identified in Stage 1, rather than a normative goal or objective, that ought to have been achieved irrespective of ecological truths and realities. In this way, the ideas of economic efficiency and feasibility are directly baked into every possible approach to arriving at an achievable mission or a conceptualized vision, as long as Stages 1, 2, and 3, are in concordance with each other. Stage 4, represents the constructed or realized, *abstract-whole.*

However, when a need for doing "free-form" designs or research, in an exploratory manner, is asserted during Stage 1 of building or operating a new system, the desire to be spontaneous, novel, and innovative can produce "unexpected" results. As such, if additional constraints concerning ecological well-being are identified and implemented via Stages 2 and 3, the eventual results can at least be safe and sound, while limiting wastage and potential harms from exploratory research and development (R&D) endeavors. 

This is why, mature companies tend to allow nascent startup founders to eagerly take risks of being spontaneous, artistic, and innovative, so that large corporations can eventually learn from the mistakes of startups, or simply buy out a surviving new enterprise that develops a competent business model with a legitimate value proposition. A startup that intends to outmatch mature competitors, would need to be able to continuously evaluate its capacity to do so, while growing its market capitalization via its optimized R&D and business operations. Principled Thinking, can help identify those required optimization strategies and techniques. 

More importantly, it can be observed that: 

>The premeditated moral consideration to prioritize exploratory and hazardous risk taking behaviors while prospecting for any types of advantages or gains, above the safety and well-being of bystanders and consumers, or above societal concerns for environmental sustainability, is an engineering and managerial decision that can often be hidden by wrongdoers, using glossy marketing materials and cleverly worded "legal disclaimers." 

Wrongdoers may even try to evade responsibility for their untoward and careless actions taken during R&D cycles, by claiming that the process of producing and operating a desired system, can be "too pedantic," if it is conducted by using formal methods of simulation and modeling. 

Identifying those types of harmful behaviors of a company's leadership and management, that are erroneous, destructive, predatory, parasitic, debilitating, or injurious in any significant manner to any persons or groups, or to any forms of natural heritage that do not exclusively belong to prospectors, requires such principled analyses. The rational and rightful measures, which can then be taken up by *litigators and prosecution teams* to penalize offending parties, and also to deter further harms that are being or can be committed by other groups of prospectors, can thus be correctly legislated via the analytical and scientific methods demonstrated by Principled Thinking. 

Even more importantly, concepts like "acceptable collateral damage" and "casualties of warfare", can be defined in a pragmatic and correct way by "constitutionally recognized entities" using Principled Thinking, in the context of authorized military, para-military, or policing activities, while growing or maintaining their "sphere of influence."[^2]

</details>


<details><summary><h3>1.3.5 Method for Generating a Machine-readable Semantic Model</h3></summary>  


<details><summary><h4><ins>Step 1: Clearing up philosophical issues</ins></h4></summary>

If you were to look up the meaning of the concept of a "word" in a regular English dictionary, it would explain that entry with words, just the same way it explains all possible entries in itself, with words. Also, typical definitions of a "word" in regular English dictionaries happen to use the concepts of a "concept" and an "idea." What then is a concept or an idea?

Now, just for fun, do look up the definition of a "dictionary", within a regular English dictionary. 

The above exercise or thought experiment, is for the sake of understanding that human beings do tend to understand the meaning of words like, "words", "concepts", "constructs", "notions", "ideas", "relationships", "edges", "nodes", "graphs", "sets", "items", "things", "containers", "systems", and "dictionaries" through physical interactions with a sociological as well as a technological environment. 

>Thus, "a socio-technical environment" is a synonym for "human ecology", which emphasizes the collaborative nature of human development and evolution through social interactions within a community that is constantly mediated by: various human made tools and technologies apart from natural structures found in the universe. 

For the sake of this overall exercise in building a model of a "semantic space", let us assert that various structures and phenomena already found in nature, such as: water, earth, sky, air, thermal changes, living creatures, etc. are easily accessible to all human beings as soon as a human is born into this universe as an infant. Obviously, infants need not learn how to use a regular dictionary of any conceivable language, to merely start living and learning from a socio-technical environment. 

Moreover, any human being at infancy, isn't a "blank slate." We know that now, and we are better informed at this point in history compared to earlier philosophers, due to our knowledge about how information is encoded within biological building blocks of living organisms such as chromosomes, because of which, a new born infant is already in possession of various types of "innate", "intrinsic", and "inherent" knowledge. That knowledge is indeed necessary for bodily functions which allow the infant to bump into things, and too start interacting with a socio-technical environment while fulfilling various needs to sustain life. 

But too often, in literature concerning physiology or psychology: 

- The idea of *"sensation"* is used for describing the process by which physical data from a socio-technical environment is translated as a signal or a "stimulus", into reactions by human sensory organs at a tissue level of organization, within microsecond intervals of time. 

- Subsequently, the interpretation of that reaction during a short period of time at the scale of seconds, to produce some type of a "mental model" along with a gain in knowledge about a "situation", at least at a basic level of understanding of "reality" due to impinging stimuli, is called *"perception."* 

- Following the processes of sensation and perception, a more complex process by which a (human) being takes those sensations and perceptions to generate inferences via different "types of reasoning", along with additional knowledge available to the *"interpreter"* via their memory, is called (in-situ or embodied) *"cognition."* 

- Some social scientists and cognitive science philosophers like to further distinguish a type of cognition, that occurs at the interface of multiple cognitive beings within an ecology, known as *extended cognition,* in comparison with the "in vivo" forms of cognitive processes. (Please see, [Distributed Cognition](https://en.wikipedia.org/wiki/Distributed_cognition), [Extended Mind Thesis](https://en.wikipedia.org/wiki/Extended_mind_thesis), and [Externalism](https://en.wikipedia.org/wiki/Externalism))

The above-mentioned ways of talking about cognition are common among previous generations of researchers, especially Anglo-Saxon and westernized ones, who have ardently tried to delineate sensation, perception, cognition, embodied cognition, and extended cognition as a sequence of processes, wherein, the ability to perform "advanced" cognitive tasks, is supposed to improve with biological development and socio-technical experiences involving "education." 

The main challenge with the above-mentioned approach to physiology, psychology, and sociology has been the inability to arrive at sensations, perceptions, and cognitive decisions or judgments about the concept of a "mind", which different groups of people can readily agree upon and utilize in political sciences, medical sciences, linguistics, religion, and other fields of study that are dependent on findings from physiology and psychology. Of course, physiology and psychology within themselves can be acknowledged as being dependent on physics, chemistry, biology, and upon various mathematical constraints of geometry to discovering knowledge. 

One might even ponder, are all of physical sciences and mathematics, including everything pertaining to human beings and the rest of the universe, contingent on a super-natural deity such as God? Which god you ask? Well, try one of these conceptions of God or a deity - https://en.wikipedia.org/wiki/Conceptions_of_God

As such, if one were to either say that [gnosis](https://en.wikipedia.org/wiki/Gnosis) is only possible via some type of [praxis](https://en.wikipedia.org/wiki/Praxis_(process)), or that gnosis is even possible via innate knowledge without any type of praxis, rituals, practices, and apologetics(https://en.wikipedia.org/wiki/Apologetics), then, in both cases, that person's worldview and consequent behaviors might not be irrational, irregular, bizarre, weird, inane, stupid, insane, crazy, deluded, or deranged in any sense of the words: irrational, irregular, bizarre, weird, inane, stupid, insane, crazy, deluded, and deranged. 

Comparatively, even the worldviews from atheism aren't somehow "scientific" or more rational than worldviews involving religious concepts. This is because atheism requires the belief that absence of precise visceral evidence of a divine phenomenon, as experienced by an individual, is in itself, necessary and sufficient evidence or proof of complete absence of all divine phenomena throughout the universe. Atheism as a socio-political enterprise, further requires that the individual who has never experienced any type of a divine phenomena in a cogent manner, conclude that all other human beings who have ever claimed to have felt something divine, were simply lying or happened to be deluded. That type of a self-centered view on knowledge excavation and learning, which takes an atheist's evolving biological development as the ultimate standard for adjudicating truthfulness of other people's experiences, presumes absolute perfection in the atheistic individual's finite abilities in mobility, awareness, and comprehension, that too with respect to a limited ecology, regardless of the stage of the atheistic person's biological development and type of ecology being considered. 

Unfortunately, a great number of people who feel that a particular god or a deity exists, and has manifested to them, or has communicated to them via some type of a phenomena, to be truthful, also behave in a presumptive manner in ascribing their individual experience as a standard for what other people ought to be able to experience and acknowledge as "faultless actuality." The socio-political enterprise of religiosity has remained fraught with massacres and molestation of many peoples as well as entire civilizations, in the name of a god, just the same way sports hooligans injure people and destroy or damage public properties, in the name of supporting athletes as fans, or the way invading groups of people wage skirmishes and even wars in the name of nationalism or patriotism.   

One must also be aware and note that, ascribing derogatory or debasing words to persons who have a different perspective or an idiosyncratic worldview, is usually done by those who have an agenda of asserting their own worldview as being superior and more legitimate, compared to all the ones they happen to disagree with. This type of crass and obscene tactic typically involves attempting to hurl insults, injuries, and destructive attacks upon other persons who have different cultural values, worldviews, and interests, for the purposes of obtaining or maintaining socio-political power and influence.   

But more importantly, for our purpose of constructing an efficiently machine-readable model of humanity's semantic space, we are to build a set of methods for parsing "digitally recorded and accessible" conceptions generated by any human being in any point of humanity's global-scale evolution, regardless of that concept's abstractness or materialistic tangibleness to different "cognizers." The chore of differentiating or distinguishing truthful and useful conceptions with respect to chosen standards for accuracy and precision, can then be carried out using the built and stored semantic space. 

Therefore, the archaic approaches used in describing the physical world around us, and our human faculties in comprehending the world, via philosophical ideas like ["tabula rasa"](https://en.wikipedia.org/wiki/Tabula_rasa), are inefficient for deciding what counts as ontic versus epistemic. More specifically, the concept of tabula rasa does not help in explain how a person's individualized semantic space starts to come into existence, to then develop over the person's lifetime. (Please see, this video on ["Birth of a Word"](https://youtu.be/RE4ce4mexrU) for a developmental psychology based perspective on discovery and application of knowledge that also indicates the need to discards the idea of tabula rasa from modern psychology.)

Worst of all, the use of ideas like "children are a blank slate", have too often been used for justifying abductions of children during cultural genocides, for the purposes of indoctrinating those children and forcibly assimilating them into a community that is different from their original ethnic and cultural background. History provides testimony about how puritanical colonialists have tried to justify genocides of indigenous cultures by claiming that forced internment and indoctrination did not do any harm or cause any pain, even while cunningly or forcibly wiping out the personality traits and cultural outlook of captive children and young individuals, particularly in the name of providing 'superior education and development' to snared people. Nefarious and insidious wrongdoers have continued to make those types of claims on the assumption that at the time of being interned or indoctrinated, the forcibly taken children and young individuals were practically a blank slate due to which, they could not have been harmed or injured in any way. 

So, instead of using ideas that have been one of the main rationales for justifying cruelty inflicted upon indigenous populations and peoples of color, we can simply decide and assert that all forms of signal processing accomplished by any system is defined as "computation." And thereby, *human cognition in its psychological form* with personality traits, as well as motivations, emotions, and thoughts, happens to be due to a dynamic cascade of computations performed by biological components of the human body. Concurrently, *human cognition in its sociological form,* which also impacts motivations, emotions and thoughts, in addition to causing evolutionary development of personality traits as well as behavioral outlook through "linguistic exchanges or interactions" and "groups based decision making", happens to involve a series of concordant computations performed by a network of people and other things, in a socio-technical environment. 

We must put aside the chore of describing what counts as an immoral, unethical, pathological, or a criminal behavior with respect to an environment as well as a predefined set of "acceptable" functions or behaviors, for a later stage of this overall endeavor. We will be able to better address that laborious work, after collecting models of individualistic and community based semantic spaces that can be agglomerated to produce *humanity's semantic space.* Defining what counts as a fault or an error, even upon encountering completely novel functions or behaviors of a given system, specifically in a way that can be machine-readable, is going to be the most lucrative aspect of this overall exercise. 

Additionally, for the sake of this discussion, we will avoid the definition of "memory" from common English vernacular, and instead use the assertion that *"memory"* exists, by virtue of adjacency of things. Therefore, the adjacency of atoms is a type of memory at a given instance of spacetime, and so is the adjacency between two trees or two cars in any part of the world, irrespective of the distances between them, aside from the adjacency between the distance-wise permutation of people and celestial objects. In fact, even the permutation of letters, gaps, and colors in this write up, is a type of memory. Indeed, all portions of the universe constitute a memory, at each instance of spacetime, but not from an egocentric or an anthropocentric perspective. Here, the technical concept of memory is being defined as a fundamental physical phenomenon that exists due to any span of adjacency, between things that already exist in the universe. Let me repeat this for emphasis: Here, it is being asserted that any configuration arising from any collection of things within spacetime, is memory. 

Next, we can define computation as a change in memory. As such, any change in configuration of adjacent things, is a computation.

So now, we can do away with philosophies involving "tabula rasa" of human mind at infancy or youth, by suggesting that there already exists an evolutionary process involving cascades of computation, across and throughout spacetime, irrespective of the existence of humans or human cognition. 

The existence of cascades of computation, across and throughout the universe at every span of its structural and functional organization, is empirically verifiable. It must be noted that concepts like "recorded history" and "archaeology", cannot be defined without such a contiguous and continuous evolutionary process of computations across spacetime, which "communicate" or "transfer information", from one part of the universe to another. So, if none of the portions of the universe are "blank" and devoid of information (as well as information transfer), then, no organism or a system within the universe as its portion, is ever devoid of information and pre-processed knowledge. 

That universal process involving the exchange of information via physical transformations and transmissions, of energy and matter, throughout all of spacetime measured in eons over intergalactic distances, has existed prior to the ["Holocene era"](https://en.wikipedia.org/wiki/Holocene_calendar#Conversion), and is going to continue to exist even after the destruction of the solar system in billions of Earth-years from now. Of course, such a non-egocentric idea is simply untrue or false, for those who believe that the universe along with all of its trappings comes into existence when they are born, and goes out of existence when they die. People who believe in that type of a purely egocentric notion of cosmology, which is limited to their individual lifespan, also aren't deranged or "mentally damaged", because a thing known as "death" has to be incorporated as an input condition, for any kind of empirical verification of different worldviews involving [eschatology](https://en.wikipedia.org/wiki/Eschatology). But, death of the observer, seems to prevent empirical verification about things that can subsequently be shared with living beings.  

Perhaps, due to topics like death, the most difficult concept to include within a semantic space, is that of "spiritual faith" and "devotion." 

I will attempt to individuate them none-the-less, by stating that spiritual faith is an extrapolation of "trust" in an entity or an event, whereby the entity or the event one has faith in, is "divine", or "supernatural." The ideas of divine and supernatural, in turn, require a person to admit that things unconstrained by physical laws of nature do exist, or can exist, within a realm that is outside the bounds of physical universe, or at least outside the bounds of known scientific knowledge. 

Answering how, and why are divine or supernatural things able to impact natural entities and events within the bounds of a physical universe that supposedly, isn't divine in the least bit, requires a whole lot of imagination and at least one [mythological](https://en.wikipedia.org/wiki/Myth#Mythology) narrative. 

It is indeed easier for some people to decide that the natural universe is divine, at an infinitesimal level of its structure and function, as well as in its entirety. Such an idea alludes to the "imminence" of divinity as an integral part of each being within the universe. There is also the idea that [divinity](https://en.wikipedia.org/wiki/Divinity#Uses_in_religious_discourse) is ever present, and that it abounds every portion of the universe, but is merely more complicated and tedious to comprehend than quantum mechanics and the square root of minus one. Most people, can do very well in life, without ever having to deal with quantum physics and complex numbers, so, similarly, religious topics about what is or isn't sacred, might also elude people, without those people suffering any physical, socio-economic, or political losses. 

Thereafter, if one understands the increasing level of attention and effort that goes into being "interested", "committed", and "dedicated" to something or someone, then they will be able to understand that "devotion" is the next logical level in the progressive scale of: interest, commitment, and dedication. 

>One might even ask, can a human being possibly obtain and retain any precepts consciously, with zero level of attention and interest invested into obtaining and then retaining said precepts? 

So, how much imagination and commitment must be applied when doing something (or anything)? 

Where must a precept from a mythological narrative or a popularly scientific journal be referenced? And when must certain types of concepts be alluded to in a conversation or a publication? Well, answers to all such questions depend on the socio-political context of a person's existing situation and their capabilities in articulating themselves. 

If you, as a mere person living in this world, were to utter certain topics out-loud (or publish them in a publicly available media), which were to then somehow causes you to get attacked overtly or covertly, or immediately, or perniciously by particular groups of people, then, hopefully you knew how to defend yourself from those kinds of attacks and attackers before doing your chosen range of activities. However, just in case, you committed certain activities unknowingly, without deliberation, without properly evaluating the consequences of those actions, or without appropriate anticipation of future outcomes of your deeds, then hopefully, you will gain an opportunity to learn from your experiences to do things differently thereon, upon withstanding the consequences of your actions. The same hope, may also be recognized in withstanding consequences generated from other people's actions, and also from those generated by non-human phenomena in nature, all of which, might not have had any pertinence to your existence before those consequences impinged upon your life and belongings. 

What then, does the concept of "hope" actually mean? Is it mere an expectation about a future possibility? Based on what? Based on lack of accurate information and precise knowledge, you say? 

Then faith, as well as trust, are practically an extension of that hope and reliance on potential outcomes in situations involving greater levels of uncertainty â€” a level much greater than the types of uncertainties that might have caused you to experience a sense of hope or reliance in yourself, or in anything else, at any point during your mortal lifespan. 

Now, would you like to say that "uncertainty" isn't an ontic thing pertaining to every portion of the physical universe we exist in? 

"Entropy" by the way, is a measure of uncertainty within a region of the physical universe due to a particular rate of information transfer between that region of spacetime and an "observer" coupled to it. So, after defining concepts like "measurability", "measure space", "flux", and an observer's capacity to observe as well as record things, we can conveniently explain how "entropic", the universe is, at a given location of spacetime. It is just that, defining an observer or things like measurability, aren't easy, but also not impossible.

Just in case you thought that religion, physical sciences, mathematics, civics, languages, music, medicine, sports, economics, or socio-politics were difficult to comprehend and practice, then you merely needed to learn a few simple strategies like Principled Thinking, for acquiring basic building blocks of knowledge and understanding, to ease your burdens, so that you may face all other things in a suitable manner; things which could come your way in this life, or in the hereafter.

</details> 


<h4><ins>Step 2: Gathering building blocks of a graph</ins></h4>

The first thing we will need is a dataset of words, which is typically a [corpus](https://en.wikipedia.org/wiki/Text_corpus) in the form of a data type known as a "dictionary", or a set of corpora such as books and articles found in [ArXiv](https://info.arxiv.org/about/index.html), or [Gutenberg Project](https://www.gutenberg.org/), or [Wikimedia](https://meta.wikimedia.org/wiki/Our_projects). 

Scraping websites to build corpora based, large language models (LLMs), is an expensive chore, but it might be a necessary task when creating a customized corpus of text for non-English languages or for other commercial applications of LLMs. 

- An example for creating a corpus in Mandarin language by scraping different websites for Named Entity Extraction, Event Extraction, and Relation Extraction during production of LLMs and "Knowledge Graphs", is provided here - https://github.com/zjunlp/KnowLM 

- A brilliant example for translating a word within a file or within an online article, in real-time, is showcased here - https://github.com/filimo/ReaderTranslator#readme 

- And here is an example of working with multiple language corpora including math symbols - https://github.com/SUSYUSTC/MathTranslate 

    - This example showcases translation of English mathematics papers into Mandarin. A similar approach can be used for translating French science and mathematics journals into English, or any other language, as required. Also, such a computerized approach is necessary for translating legal documents from or to, an official language, in relation with an accessible language of discourse. 

For our purposes, let us start by using a regular English dictionary, from either one of the following options:

- The version of "Webster's Unabridged Dictionary" of English Language, available from Gutenberg Project in UTF-8 format - https://www.gutenberg.org/ebooks/29765   

- The "english Dictionary.csv" file from the repository - https://github.com/benjihillard/English-Dictionary-Database 

- And here is a smart approach for creating an "open dictionary" compatible with REST API format - https://github.com/open-dictionary/english-dictionary/ 

As well as the .txt or .json files containing "set of English words" without definitions from - https://github.com/dwyl/english-words 

Additional elementary tokens of English language such as punctuation can be obtained from the following git repository, which also has a collection of popular symbols along with alphabets and notations used in other languages - https://github.com/symbl-cc/symbl-data

It would be worthwhile to make sure that each word and irreducible token within the "set of English words", has an entry within the dictionary that is to be used in the subsequent steps. Additionally, one can find items within a given dictionary, which aren't contained within the flattened set of "unique" words, to then add that word to the flattened set, for future use in creating a mathematical graph of words. 

<b>Notes:</b> 

1. Computing languages like Python and Julia have convenient, builtin functions like `set`, `dict`, and `map` to efficiently do the above-mentioned comparisons between `lists` of items. 

2. While building an LLM from a corpus, one can use a "tokenizer" to first create a list, or a sorted list, or a flattened set of unique words found in the corpus. Doing that can also be used in building a ["concordance"](https://en.wikipedia.org/wiki/Concordance_(publishing)) of the corpus. Here is a good resource for different types of tokenizers - https://huggingface.co/docs/transformers/main_classes/tokenizer 


<h4><ins>Step 3: Building a graph that showcases dependencies among linguistic primitives</ins></h4>

We obviously need to discuss what the mathematical meaning of a graph is supposed to be. So, here is a nice video that explains necessary terminologies associated with graphs: [Introduction to Graph Theory: A Computer Science Perspective](https://youtu.be/LFKZLXVO-Dg) 

When making a graph of an entire corpus, the label of each node turns out to become an individuated "string of characters." Those individuated items are dubbed here as "linguistic primitives", which can take the form of: 

- a punctuation; 

- a numeral; 

- letters of an alphabet, including different types of blank spaces used alongside those letters in formatting text; 

- a glyph or an emoji that forms a graphical symbol; 

- an n-gram token, which is a sub-string of a whole word containing *n* number of characters, where *n* is greater than 1; 

- a string of characters or tokens that can be recognized as being *"a proper word"* by human beings of a speech community; 

- a hyphenated compound word; 

- or a phrase that forms an idiomatic expression. 

<br>

The edges of the graph can then be represented as a list of "tuples" of connected nodes, as shown in "Graph Representation" section of the video on graph theory - https://youtu.be/LFKZLXVO-Dg?t=691. 

The entire graph of a corpus can thus be modeled using a data structure known as a dictionary in computer science, wherein, each node's label is associated with a list of other nodes connected to it, at one unit distance of adjacency. You will eventually see how that becomes useful in computing "similarity" between words, during Natural Language Processing with categorization and classification tasks conducted over a given corpus. 

By the way, the technical definition of the data structure called `dictionary` in computer science, happens to be identical to the mathematical definition of a graph. 

Also, many researchers in computational linguistics like to label n-gram tokens generated by permutations of alphabets, that do not constitute a proper word recognizable by human speakers of a language, as non-words. Consequently, they like to ignore or delete non-words, during analytical tasks. However, hardcore cryptographers tend to treat all strings of characters formed by any permutation of linguistic primitives, within a received or an intercepted message, as objects that are worthy of intrigue and analysis.  

Let's go over an example to better understand the above set of ideas:

- Let the given corpus be: 

    ```
        "A shrub in the desert is a tree. 
        And to a dust-mite in the desert, 
        The shrub might as well be an entire forest."

    'Tokenizing' special characters in a set of strings,
    can be challenging. However, numerals like: 765239 
    can be handled more conveniently. On the other hand,
    a defined value such as 2.220446049250313e-16 might not 
    get parsed correctly. And hopefully you obtained the given
    text with suitable line-endings in the form of \n 
    or an empty space such as "", instead of a
    \r\n or a sudden 0x1A in the middle of the text.
    
    Additionally, make sure that white spaces didn't encode
    some kind of a special meaning.

    Figuring out what counts as a typesetting error, misspelling, 
    typo, or a grammatical slip-up versus an encoded message, 
    is an important task, that can be taken up

    in a later stage of analysis.
    
    Having said that, a coder's or a code breaker's ability to notice
    trends or repeating cellular structures known as "patterns"
    is key to parsing, and thereafter interpreting the given text,
    or a sample of signals. 
    ```

- We can parse the given corpus using a tokenizer, to obtain a list of its linguistic primitives, as shown in the following Python Notebook. The ipynb file also shows how to obtain a frequency count of the words in the given text.

    - py notebook code for parsing given text - https://github.com/callthis/status-quo/tree/main/docs/nlp_example

    - To run the files of this example, you can load them into [Google Colab,](https://colab.research.google.com/) or run them locally after downloading them, using a suitable [Python development environment.](https://realpython.com/python-virtual-environments-a-primer/) 
    
- We then graph the contents of the dictionary to obtain *Diagram 1.*



**TODO:** mermaidjs code for generating the diagram 

While the above example is shown with a tiny sample of text, this kind of a procedure can be carried out on a larger corpus such as a list of regular English words from *Step 2,* in order to obtain *Diagram 2.*  


<h4><ins>Step 4: Restructuring the graph using the method of Principled Thinking</ins></h4>

MORE TODO.

</details>


<details><summary><h3>1.3.5 Proposed Applications of Semantic Models</h3></summary>

Concepts like artificial intelligence, strong artificial intelligence (AI) and general intelligence (GI) are frequently used in fields of humanities, business, engineering, and sciences. The definitions of, and differences among concepts like strong AI and GI are debatable. To aid current cybernetic researchers, the proposed schema of a machine-readable semantic model can accommodate knowable concepts, and compare all forms of "intelligent agents." An intelligent agent may be a machine, a living being, a combination of such entities or even a collection of such combinations. In this manner, information processing capabilities between agents can be compared for the sake of systems design and efficient work allocation among agents.

</details>

---

<em>Work in Progress (WIP)</em>


[^1]: This drawing from 27th December, 2015, was made by my ex-wife who was also my best friend. She was the only person to immediately understand the model of a semantic space, that I had tried to explain to her using mere words. I wouldn't have been able to write about this topic, without her drawing illustrating the method to obtain a 3-D model of a semantic space, from a 2-D model of an abstraction hierarchy. In the year 2020, the relationship I had with my best friend and life partner was destroyed by Canadian operatives who defiled our home. Canada has yet to apologize and make amends for the devastating wrongdoings of its agencies, which caused our lives to be torn apart. Worse, such ruinous harms have continued to be inflicted by Canada's state-sponsored agents and contractors upon targeted groups and individuals, because of xenophobia, Islamophobia, racism, and discriminatory prejudices that are endemic to the organizational culture of key institutes in Canada. 

[^2]: I am going to guess that various agents and agencies from Canada will eventually try to claim that the lives of my family and I, similar to those of many others, were 'acceptable collateral damage', as per the designs and agenda of Canadian national security and secrecy. The unresolved conflict, isn't with their post-hoc patriotism, it is with their ongoing, premeditated, coordinated, bad-faith behaviors borne out of their long-standing hypocrisy, conceit, ignorance, bigotry, racist ideologies, and Islamophobic world views; all of which, happen to be nefarious and insidious! So, it is naturally the responsibility and duty of other conscientious agents and agencies, including those from Canada, to immediately rectify and deter those types of bigoted bad-faith behaviors, with proper remedies and punitive damages awarded to survivors and victims. 