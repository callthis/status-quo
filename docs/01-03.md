<div align="center">
  
  **[:arrow_left: Previous Section][Prev] | [Table of Contents][TOC] | [Next Section :arrow_right:][Next]**
  
</div>

---

## 1.3 Automated Discovery and Exploitation of Zero-day Faults

For this mathematics and computer science based topic, I will first describe a few definitions, and present an example that introduces the challenges involved in automated detection of human faces within a video. 

The presented example is for giving the reader an idea about the difficulty in automatically "recognizing", any conceivable "concept", in any possible media and ecology, within which an Artificial Intelligence (AI) based system can co-exist and function, along side beings with general intelligence. 

Ultimately, this section aims to describe an automated method for discovering aspects of any given system, that constitute a "fault", and that too a zero-day fault, to subsequently generate automated solutions for patching them by replacing the faulty system with a better, automatically generated architecture; or for exploiting the identified faults, based on morality and ethics of learned commercial, industrial, and military purposes and activities. 

How can building such an advanced AI, not turn into a bane for humanity? Well, we will just have to figure out answers to such concerns using advanced AI that can evaluate itself, other AI, as well as non-AI based systems presented to it, including ones that it finds autonomously because doing all of that manually, is not feasible within one human lifetime of any human being. Would putting more "men" on the job, achieve the required task of answering difficult questions pertaining to AI ethics? Maybe. But how would one find and task sufficiently many people from all walks of life, and genders, to contribute to answering such questions at a publicly accessible platform? 

By the way, if you can identify something within yourself, or in your environment, that is "not a system", then, please do contact me, by submitting a pull request with your educational article or content, regarding the concept of not-system, which can then be added to this repository in a collaborative manner. Or you may join the online discussions here - https://github.com/orgs/callthis/discussions 

**Note:** All contents of public discussions pertaining to the GitHub "Org" named "callthis" are attributed the license of Creative Commons Zero, Version 1.0 (CC0).

### 1.3.1 Preliminaries 

Let "point" is a thing such that, it has no further parts, not even a causal factor that generates or justifies its existence. 

The above definition of a point from Euclid's Elements, turns out to be one of the most brilliant axioms that any human being could have ever discovered, because it forms the basis of all geometry, and is an intrinsic part of every aspect of space-time as well as all other gamuts. Such a definition of a point also indicates that it is whole within itself. Furthermore, due to its imperceptible form and infinitesimal nature, it is an abstract mathematical entity. 

One can also assert that infinitely many points exist between any two adjacent points. But, as human beings, we are compelled to use a "dot" to represent a point, when discussing it in the finite context of a tangible medium such as a piece of paper, a chalk board, a cuboid, or an ellipsoid. 

Now, any "collection" of points can be termed as a locus. A straight line is a locus in which the collection of points share a property called linearity. Any line, along which and about which, a "set" of points is distributed, is called a geometrical axis. The number of mutually "orthogonal" axes needed to unambiguously index each point within a generalized "space" indicates the "dimensionality" of that space. The maximum and minimum span that can be measured within a space, respectively define the maximum "size", and the "least count" of mathematical operations that produce a measure within, or from the space being taken into account. 

In mathematical literature about machine learning, the concept of "dimension", of a given matrix, is often confused with the "dimensionality" of the matrix. Also, in literature about machine learning, the number of dimensions in a trained model, usually alludes to the number of "features" obtained via permutations of all the elements available in the training and test datasets. 

So, to avoid confusion, let us instead use the words "size", and ["tensor rank"](https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)#Tensor_rank), to describe a set of points, elements, data, or items that can be represented as a matrix, whereby: 

- The size of a given matrix is the total number of elements within the matrix. 

- The tensor rank of a given matrix coincides with the total number of ["Eigenvalues"](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Calculation) of the dataset described within the matrix. 


### 1.3.2 An Example from The Science of Image Processing

As an example, a trained machine learning model that [detects human faces](https://en.wikipedia.org/wiki/Face_detection) within a RGB-color video, might be described using a matrix that could have a so-called dimension of billions. Billions of what, you ask? That kind of a question only tends to upset some of the machine learning scientists and engineers who build machine learning models, which then causes them to fall into an argument about whether the word dimension of their training algorithm's output matrix alludes to its size, or to its maximum number of columns. A dataset containing a RGB-color video only has elements distributed: 

- along one axis describing the color channel of a pixel; 

    - the integer values on this axis indicate a number corresponding to a color channel, that is, red, green, or blue

- about two more axes for describing the position of a pixel in an image frame 

    - the integer values with respect to these two axes correspond to a coordinate of a pixel, where each pixel is bounded within the size of the image frame, say, 1080x960 pixels 

- along one more axis for the light intensity at a given pixel location indexed by the above three axes 

    - a pixel's intensity is typically an integer value between 0 and 255, for images that use 32 bit data in an image container format, like [JPEG](https://en.wikipedia.org/wiki/JPEG#Typical_use) 

- and along one more axis for describing the time-stamp of each image frame in a video container format, like [MPEG](https://en.wikipedia.org/wiki/Moving_Picture_Experts_Group)

    - this integer value is dependent on the least count of the clock used for timing the video, which could be in milliseconds, resulting in, say, 60000 frames for a one minute video  

So, in the above example, the size of the dataset would be 3x1080x960x256x60000 = 4.7775744e+13 *pixels.* 

To then say that you have a training dataset matrix with approximately 47 billion dimensions would be silly, because regardless of the change in number of video frames, any pixel in the dataset would be unambiguously indexed with only five pieces of knowledge about its location and intensity. As such, the video data would be distributed in a space constructed using five axes, that is, a geometric space having a tensor rank of five. 

A trained machine learning model for face-identification could be a matrix with a very large size, due to a particular permutation of, say, [Haar-like Features](https://en.wikipedia.org/wiki/Haar-like_feature) used for describing a human face to the [Viola-Jones algorithm.](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework) However, that trained model or template for face-identification, can use the intensity values of a single color channel, for each two-dimensional video frame. Thus the data in the trained model would instead be described by a space constructed by three axes for pixel locations within an image frame, and one axis for the time-stamp of each image frame in the video stream. 

That trained model or template, can then be iteratively matched with "chunks" or sections of each test image frame, to be able to identify any geometry that resembles a human face. Naturally, the amount of computational resources needed for identifying faces within a live video stream, would be quite expensive.

So, imagine doing the above type of "feature-identification" within live audio and chat data-streams alongside live video data, for building an interactive robot; a robot which can understand as well as interact with the world around it, in a manner that is as intelligent as a college educated adult human being, if not better. Even without haptic and olfactory data channels, merely audio and visual channels expressed as electromagnetic signals to a robot, to make it as "self-actualized" as a college educated adult human being, may seem like a physically impossible or infeasible project. And yet, one needs to remember that something like the products being created by Google or Baidu, already outperform human beings on various well-defined tasks that can be digitized. 

Thus, the process of merely identifying building blocks or factorized components, of a digitized data-stream from "the real world", which resemble labeled items in a training dataset, isn't the required final output of a robotic system that is meant to approach "general intelligence." The algorithmic procedure for making the AI recognize semantic meaning out of a string of features within a data-stream, to then "autonomously" act upon "the knowledge and understanding" obtained from that process, firstly requires a [Large Language Model](https://en.wikipedia.org/wiki/Large_language_model). 

From philosophies of mind and natural language, as taught in Westernized universities, oral or written human speech that makes use of "words" or "symbols", not only conveys pre-formulated and formatted intent of the communicator, it also conveys the "intentionality" that the communicator experiences, during the act of performing activities in-line with the communicator's motives and desires. Said intentionality is supposed to be the "feeling", or human experience, of wanting to continue or to halt an ongoing activity, based on real-time sensory feedback from the communicator's environment. That philosophical approach to describing linguistics and cognition is drastically limited. So, let us generalize the idea of "language" from its gestural, or behavioral form, to a cascade of synchronous as well as asynchronous articulations that can provide "signals" from a sender to a receiver, via the physical process of "communication." 

>The most basic definition of physical communication is: "transfer of 'information' from one system to another via signal transmissions, energy transduction, or changes in momentum or electromagnetic states of the interacting systems via 'contact', at any possible span of short or long distances in space-time." 

At least, that is how I like to define physical communication. And, changes in momentum or electromagnetic states necessarily entail thermodynamic changes among interacting systems, that are causal, and aren't merely coincidental. 

What then is a system, and how are system boundaries or interfaces defined? Well, only "nothingness" is not a system, and the chore of identifying dynamically evolving boundaries of a system, requires the use of ["Ontological Methods in Systems Engineering."](https://github.com/callthis/status-quo/blob/main/docs/01-02-07.md#1271-ontological-methods-in-systems-engineering)

So, let us construct a proper "manifold" that describes the mathematical relationships between all conceivable concepts, with those concepts being distributed upon the constructed manifold, including the concept of a concept, the concept of recursion, and even the concept of what a manifold is supposed to be in terms of a mathematical object of study. In doing so, we will achieve an algorithmically generated "semantic model", instead of a "language model." The semantic model can then be utilized for tasks like discovering and studying mathematical theorems, that have yet to be recognized using human efforts without the aid of generative-AI based analytical engines.  

<br>

As shown in the following diagram, ***Line AB*** represents the axis defined as "the part-whole continuum", and ***Line AD*** represents the axis defined as "the abstract-tangible continuum." The 2-Dimensional ***Area ABCD,*** represents a gamut of concepts known as an ["abstraction hierarchy."](https://github.com/my-realm/oc/blob/master/doc/ah.md#history-of-abstraction-hierarchy) Each point on this gamut is indexed as the coordinate of an individuated concept, such that each of those concepts are comprehensible to human beings. The semantic meaning ascribed to each concept on the gamut, can thus, only be relatively abstract-or-tangible with respect to a subjective observer; and is simultaneously, either a component or an ensemble, in relationship to other concepts located on the gamut, using the objective basis of "set theory." 

This 2-Dimensional gamut of concepts can then be converted into a 3-Dimensional model, by introducing an axis that represents the continuum of "depth-feature versus surface-feature." 

So, to make the corners of the 2-Dimensional ***Area ABCD*** touch at a single ***Point E,*** "fold" the area as shown in the following diagram, to create a 3-Dimensional volume that represents, "a semantic space."[^1]  

<p align="center">
    <img width="65%" src="../imgs/semantic_space-3d-model.png"></img>
    <br>
    <b>Converting the 2-D model of an "abstraction hierarchy" into a 3-D model of a "semantic space."</b> 
</p>
<br>

In the newly obtained 3-D model of a semantic space: 

- Depth-feature is a thing, or a concept, that is closer to the sub-atomic description of reality in terms of physical space-time measured in spans of [Natural Units](https://en.wikipedia.org/wiki/Natural_units) (such as but not limited to: [Planck units](https://en.wikipedia.org/wiki/Planck_units), [Stoney Units](https://en.wikipedia.org/wiki/Stoney_units), and [Fine-Structure Constant](https://en.wikipedia.org/wiki/Fine-structure_constant)), in comparison to the macro-level, biological shape and size of human beings. 

- Surface-feature is a concept, or a thing, that is more readily accessible to naked human biological sensory organs, and thus to human cognitive faculties, without the aid of technological instruments, or tools and gauges. 

- And the topic of abstract-versus-tangible can be made more objectively measurable, by comparing all given concepts, including things like "time", or "chair", with the most abstract concept called "point." Let the origin of the new 3-Dimensional coordinate system containing the model of a semantic space, coincide with the **Point E.** The origin of the coordinate system is also supposed to represent the location of the geometrical concept of "a point." All other concept placed on the 3-Dimensional manifold are to be measured at a distance with respect to the origin using vector algebra. How to do so will be made clear in the subsequent sub-sections. 

---

<details><summary>By the way,</summary> don't try comparing things to God, either via similarity or via contrast, because all forms of comparison of a conceivable thing with God, results in that created thing ultimately becoming, annihilated.</details> 

---

Anchoring concepts to locations on a 3-Dimensional manifold called the semantic space, using a regularized arrangement that can be codified, highlights causal relationships due to which, causality is directed from a relatively abstract depth-feature to a relatively tangible surface-feature. Subsequently, any feedback loops that can exist in a vectored manner, directed from a relatively tangible surface-feature to a seemingly abstract depth-feature, can also be discovered via the encoding method that is described in the next sub-section of this article. Therefore, causality exists within ecological feedback loops, and isn't something that is somehow directed strictly from "the human mind", onto "the material world." 

So, one must first ask themselves, are there any combinatorial arrangements of physically measurable quantities of the universe we exist in, that are capable of accurately and precisely describing concepts like personhood, cognition, intelligence, awareness, spirituality, attention, willingness, wellness, aesthetics, morality, ethics, veridicality, legality, meaningfulness, or contentment? 

Also, do people need to define concepts like mind, or a soul, the way the concepts of point, or the average [speed of light](https://en.wikipedia.org/wiki/Speed_of_light) between two points within "isotropic" "free space" have been defined, in order to be able to express themselves as mere human beings; or to be able to construct scientific tools and equipment for experiencing a better quality of life, while utilizing other well-defined concepts via arts as well as engineering and managerial sciences? 

Most importantly, what kinds of concepts are yet to be properly defined or even discovered, that would otherwise make existence more sensible and worthwhile, for human beings as well as other living species?  

### 1.3.3 Intuitive Method of Constructing a Semantic Model of a Given System 

We can take a look at an intuitive form of the pseudo-code for creating a machine-readable model of a semantic space, using the following diagram labeled as "Principled Thinking."  

<br>
<p align="center">
    <img width="65%" src="../imgs/Principled-Thinking.png"></img>
    <br>
    <h4 align="center">Principled Thinking</h4> 
</p>
<br>

The above diagram may better explain the process of discovering "blind-spots" and "misconceptions" within any system's design, for persons who are less inclined to using an algorithmic description of how to construct a machine-readable semantic space. The diagram depicts four stages of analyzing any given system, in which each stage must be in concordance with the adjacent stages. 

- To begin the analysis of, say, an existing grocery store, the analyst would start at Stage 1, concerning the store owners' ethos and cultural values. The reason for owning and operating a grocery store would be derived from the cultural values and needs formally stated by the store's stakeholders. The roles and responsibilities that the company's employees are expected to perform, as well as the standards of customer satisfaction and yearly profits they are to aim for, are contingent on the cultural values that company members can adhere to. If concepts like "honesty", "customer satisfaction", "employee code of conduct", "facility's cleanliness", "handicap parking and accessibility", "building's architectural appeal", "product visibility", or "safety" weren't a primary concern, then the analyst would be able to identify such deficiencies before moving onto the next stage. Stage 1, provides answers to questions starting with a "why." For example, "Why is a security system needed in a grocery store?"

- Stage 2, of the store's analysis would identify the financial and physical resources available that match the principles and policies of the store's proprietors. In this stage the analyst can identify underutilized assets as well as shortages within required resources that need to at least, meet the value system and sense of aesthetics subscribed to by the company's stakeholders. Stage 2, provides answers to questions starting with a "what" or a "which." For example, "What kind of a neighborhood is the store located in? Which type of financial, physical, and digital security measures are available or needed, to maintain desired level of operational safety and 'peace of mind'?" 

- Stage 3, of the analysis provides answers to questions beginning with a "how." For example, "How is a particular version of financial, physical, and digital security system to be implemented, with the necessary administration of employee training and corporate policies?" This stage of analysis, conjoined with the previous stages, would highlight key areas of strengths and weaknesses in the day-to-day managerial operations of the grocery store. Any new polices that need to be created, and any physical resources that need to be reallocated or additionally acquired, are to be clearly identifiable during this stage. This is also the stage where business operators would be able to analyze and ratify, human resource policies, inventory management schedules, advertising campaigns, salaries and wages, product pricing strategies, and investor relationships, to arrive at statements about sales and profit goals. 

- Stage 4, which is an abstract-whole, turns out to be a natural outcome of the previous stages of "thinking and making", whereby the company's goals become defined in "achievable and realistic" terms. Any envisioned goals without taking stock of available skills and resources along with a proper grounding within the company's espoused cultural values, would most likely turn into a pipe-dream. Coming up with fancy goals and mission statements that possibly cannot be achieved within the constraints of available ground truths, can thus be avoided. Ultimately, Stage 4 of the analysis, must be in accord with the axiomatic principles identified at the very onset of the business analysis. 

You may thus note that, Stage 1 of the analysis identifies *abstract-parts* of a given system. These abstract-parts are the principles, norms, and needs, that constrain or bound the system within an ecology, in an axiomatic or "legally" defined way. Stage 2 of the analysis describes a set of *tangible-parts* of the system, pertaining to existing materials and processes that can be utilized as per known constraints. Then, Stage 3 produces a set of performance indices and designs in the form a *tangible-whole,* which describes how certain kinematic and dynamic relationships can be connected or built, to arrive at viable goals using the concordance between Stages 1 and 2. 

The eventual Stage 4, reifies the outputs of the system in a descriptive manner, by showcasing its qualities in comparison to the ecological constraints identified in Stage 1, rather than a normative objective that ought to have been achieved irrespective of ecological truths and realities. In this way, the ideas of economic efficiency and feasibility are directly baked into every possible approach to arriving at a goal, as long as Stages 1, 2, and 3, are in concordance with each other. Stage 4, represents the constructed or realized, *abstract-whole.*

However, when a need for doing "free-form" designs or research, in an exploratory manner, is asserted during Stage 1 of building or operating a new system, the desire to be spontaneous, novel, and innovative can produce "unexpected" results. As such, if additional constraints are identified via Stages 2 and 3, the eventual results can at least be safe and sound, while limiting wastage and potential harms from exploratory research and development (R&D) endeavors. 

This is why, mature companies tend to allow nascent startup founders to eagerly take risks of being spontaneous, artistic, and innovative, so that large corporations can eventually learn from the mistakes of startups, or simply buy out a surviving new enterprise that develops a competent business model with a legitimate value proposition. A startup that intends to outmatch mature competitors, would need to be able to continuously evaluate its capacity to do so, while growing its market capitalization via its R&D and business operations.  

More importantly, it can be observed that: 

>The premeditated moral consideration to prioritize exploratory and hazardous risk taking behaviors while prospecting for any types of advantages or gains, above the safety and well-being of bystanders and consumers, or above societal concerns for environmental sustainability, is an engineering and managerial decision that is often hidden by wrongdoers, using glossy marketing materials and cleverly worded "legal disclaimers." 

Wrongdoers may even try to evade responsibility for their untoward and careless actions taken during R&D cycles, by claiming that the process of producing and operating a desired system, can be "too pedantic," if it is conducted by using formal methods of simulation and modeling. 

Identifying those types of harmful behaviors of a company's leadership and management, that are erroneous, destructive, predatory, parasitic, debilitating, or injurious in any significant manner to innocent people, or to any forms of natural heritage that do not exclusively belong to prospectors, requires such principled analyses. The rational and rightful measures, which can then be taken up by prosecutors against offending parties to penalize them, and also to deter further harms that are being or can be committed by other groups of prospectors, can thus be correctly legislated via the analytical and scientific methods demonstrated by Principled Thinking. 

Even more importantly, the concepts of "acceptable collateral damage" and "casualties of warfare", can be defined in a pragmatic and correct way by "patriotic entities" using Principled Thinking, in the context of authorized military, para-military, or policing activities, while growing or maintaining their "sphere of influence."[^2]


### 1.3.4 Method for Generating a Machine-readable Semantic Model  


### 1.3.5 Proposed Applications of Semantic Models  



[^1]: This drawing from 27th December, 2015, was made by my ex-wife who was also my best friend. She was the only person to immediately understand the model of a semantic space, that I had tried to explain to her using mere words. I wouldn't have been able to write about this topic, without her drawing illustrating the method to obtain a 3-D model of a semantic space, from a 2-D model of an abstraction hierarchy. In the year 2020, the relationship I had with my best friend and life partner was destroyed by Canadian operatives who defiled our home. Canada has yet to apologize and make amends for the devastating wrongdoings of its agencies, which caused our lives to be torn apart. Worse, such ruinous harms have continued to be inflicted by Canada's state-sponsored agents and contractors upon targeted groups and individuals, because of xenophobia, Islamophobia, racism, and discriminatory prejudices that are endemic to the organizational culture of key institutes in Canada. 

[^2]: I am going to guess that various agents and agencies from Canada will eventually try to claim that the lives of my family and I, similar to those of many others, were 'acceptable collateral damage', as per the designs and agenda of needing to maintain national security and secrecy. The long-standing unresolved problem, isn't with their post-hoc patriotism, it is with their ongoing, premeditated, coordinated, bad-faith behaviors borne out of their hypocrisy, conceit, ignorance, bigotry, racist ideologies, and Islamophobic world views; all of which, happen to be nefarious and insidious! 